{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"TPU","colab":{"name":"Lab_2.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"}},"cells":[{"cell_type":"code","metadata":{"id":"e7tJA_J-8onJ"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","dir = '/content/drive/MyDrive/Courses/2021fall/Spoken Language Technologies/Lab2/'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oshEAmIA8mFR","executionInfo":{"status":"ok","timestamp":1632540003079,"user_tz":300,"elapsed":595,"user":{"displayName":"Ting-Yu Dai","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17719512292427902131"}}},"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","# Load the dataset\n","data = np.load(dir + 'lab2_dataset.npz')\n","train_feats = torch.tensor(data['train_feats'])\n","test_feats = torch.tensor(data['test_feats'])\n","train_labels = torch.tensor(data['train_labels'])\n","test_labels = torch.tensor(data['test_labels'])\n","phone_labels = data['phone_labels']\n","\n","# Set up the dataloaders\n","train_dataset = torch.utils.data.TensorDataset(train_feats, train_labels)\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True)\n","\n","test_dataset = torch.utils.data.TensorDataset(test_feats, test_labels)\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=8, shuffle=False)\n","\n","# Define the model architecture\n","class MyModel(nn.Module):\n","    def __init__(self):\n","        super(MyModel, self).__init__()\n","        self.linear2 = nn.Linear(512, 48)\n","        self.conv1 = nn.Conv1d(40, 512, 3)\n","        self.conv2 = nn.Conv1d(512, 512, 3)\n","        self.conv3 = nn.Conv1d(512, 512, 3)\n","        self.conv4 = nn.Conv1d(512, 512, 3)\n","        self.conv5 = nn.Conv1d(512, 512, 3)\n","        self.attn1 = nn.Linear(512, 9)\n","        self.attn2 = nn.Linear(512, 7)\n","        self.attn3 = nn.Linear(512, 5)\n","        self.attn4 = nn.Linear(512, 3)\n","        self.dropout = nn.Dropout()\n","        # self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n","        \n","        # TODO: Fill in the model's layers here\n","\n","    def forward(self, x):\n","        batch_size = x.shape[0]\n","        x= x.permute(0, 2, 1)\n","        embedded = F.relu(self.conv1(x)) # b, 512, 9\n","        attn_weights = F.softmax(self.attn1(embedded.permute(0, 2, 1)), dim=2) # b, 9, 9\n","        x = torch.bmm(embedded, attn_weights)\n","\n","        embedded = self.dropout(F.relu(self.conv2(x)))\n","        attn_weights = F.softmax(self.attn2(embedded.permute(0, 2, 1)), dim=2) # b, 9, 9\n","        x = torch.bmm(embedded, attn_weights)\n","\n","        x = self.dropout(F.relu(self.conv3(x)))\n","\n","        x = self.dropout(F.relu(self.conv4(x)))\n","\n","        x = self.dropout(F.relu(self.conv5(x)))\n","        x = self.linear2(x.reshape(batch_size, -1))\n","        # TODO: Fill in the forward pass here\n","        return x\n","\n","# Instantiate the model, loss function, and optimizer\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = MyModel()\n","model = model.to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n","scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[15, 30], gamma=0.1)\n","\n","def train_network(model, train_loader, criterion, optimizer, epoch):\n","    # TODO: fill in\n","    for i, (inputs, labels) in enumerate(train_loader):\n","        inputs = inputs.to(device)\n","        labels = labels.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        if i % 1000 == 0:\n","            print('Epoch %d [%d / %d]: loss: %.4f' % (epoch, i, len(train_loader), loss.item()))\n","\n","def test_network(model, test_loader):\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for data in test_loader:\n","            inputs, labels = data\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","            outputs = model(inputs)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","    print('Test accuracy: %d %%' % (100 * correct / total))\n","\n"],"execution_count":35,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zhd__Dmw-O4O","outputId":"64f06e09-0212-4ab7-a2e8-23265e8628c7"},"source":["for epoch in range(1, 41):\n","    train_network(model, train_loader, criterion, optimizer, epoch)\n","    test_network(model, test_loader)\n","    scheduler.step()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1 [0 / 5592]: loss: 5.5907\n","Epoch 1 [1000 / 5592]: loss: 3.4199\n","Epoch 1 [2000 / 5592]: loss: 2.4304\n","Epoch 1 [3000 / 5592]: loss: 2.7802\n","Epoch 1 [4000 / 5592]: loss: 4.1211\n","Epoch 1 [5000 / 5592]: loss: 3.0983\n","Test accuracy: 20 %\n","Epoch 2 [0 / 5592]: loss: 2.9204\n","Epoch 2 [1000 / 5592]: loss: 1.9015\n","Epoch 2 [2000 / 5592]: loss: 2.6517\n","Epoch 2 [3000 / 5592]: loss: 2.1312\n","Epoch 2 [4000 / 5592]: loss: 2.3306\n","Epoch 2 [5000 / 5592]: loss: 2.7180\n","Test accuracy: 31 %\n","Epoch 3 [0 / 5592]: loss: 1.7507\n","Epoch 3 [1000 / 5592]: loss: 2.2915\n","Epoch 3 [2000 / 5592]: loss: 2.3356\n","Epoch 3 [3000 / 5592]: loss: 2.9825\n","Epoch 3 [4000 / 5592]: loss: 1.9525\n","Epoch 3 [5000 / 5592]: loss: 2.2181\n","Test accuracy: 35 %\n","Epoch 4 [0 / 5592]: loss: 1.9252\n","Epoch 4 [1000 / 5592]: loss: 1.9226\n","Epoch 4 [2000 / 5592]: loss: 2.2926\n","Epoch 4 [3000 / 5592]: loss: 2.1448\n","Epoch 4 [4000 / 5592]: loss: 1.5731\n","Epoch 4 [5000 / 5592]: loss: 2.0863\n","Test accuracy: 39 %\n","Epoch 5 [0 / 5592]: loss: 1.7238\n","Epoch 5 [1000 / 5592]: loss: 1.4859\n","Epoch 5 [2000 / 5592]: loss: 1.0918\n","Epoch 5 [3000 / 5592]: loss: 2.9548\n","Epoch 5 [4000 / 5592]: loss: 2.2507\n","Epoch 5 [5000 / 5592]: loss: 1.2552\n","Test accuracy: 42 %\n","Epoch 6 [0 / 5592]: loss: 1.7637\n","Epoch 6 [1000 / 5592]: loss: 2.2933\n","Epoch 6 [2000 / 5592]: loss: 1.4460\n","Epoch 6 [3000 / 5592]: loss: 1.3824\n","Epoch 6 [4000 / 5592]: loss: 0.5137\n","Epoch 6 [5000 / 5592]: loss: 2.6795\n","Test accuracy: 45 %\n","Epoch 7 [0 / 5592]: loss: 1.2443\n","Epoch 7 [1000 / 5592]: loss: 2.4879\n","Epoch 7 [2000 / 5592]: loss: 2.6272\n","Epoch 7 [3000 / 5592]: loss: 2.4862\n","Epoch 7 [4000 / 5592]: loss: 1.0597\n","Epoch 7 [5000 / 5592]: loss: 1.8902\n","Test accuracy: 45 %\n","Epoch 8 [0 / 5592]: loss: 1.1726\n","Epoch 8 [1000 / 5592]: loss: 2.1373\n","Epoch 8 [2000 / 5592]: loss: 0.9962\n","Epoch 8 [3000 / 5592]: loss: 1.8982\n","Epoch 8 [4000 / 5592]: loss: 2.0893\n","Epoch 8 [5000 / 5592]: loss: 1.6591\n","Test accuracy: 47 %\n","Epoch 9 [0 / 5592]: loss: 1.1191\n","Epoch 9 [1000 / 5592]: loss: 1.3460\n","Epoch 9 [2000 / 5592]: loss: 1.3423\n","Epoch 9 [3000 / 5592]: loss: 1.4093\n","Epoch 9 [4000 / 5592]: loss: 1.9876\n","Epoch 9 [5000 / 5592]: loss: 1.5493\n","Test accuracy: 47 %\n","Epoch 10 [0 / 5592]: loss: 0.9813\n","Epoch 10 [1000 / 5592]: loss: 2.3092\n","Epoch 10 [2000 / 5592]: loss: 1.0441\n","Epoch 10 [3000 / 5592]: loss: 1.3019\n","Epoch 10 [4000 / 5592]: loss: 1.0942\n","Epoch 10 [5000 / 5592]: loss: 1.3711\n","Test accuracy: 49 %\n","Epoch 11 [0 / 5592]: loss: 0.9219\n","Epoch 11 [1000 / 5592]: loss: 1.0202\n","Epoch 11 [2000 / 5592]: loss: 1.6113\n","Epoch 11 [3000 / 5592]: loss: 0.5870\n","Epoch 11 [4000 / 5592]: loss: 1.5378\n","Epoch 11 [5000 / 5592]: loss: 1.1152\n","Test accuracy: 50 %\n","Epoch 12 [0 / 5592]: loss: 2.4346\n","Epoch 12 [1000 / 5592]: loss: 1.1824\n","Epoch 12 [2000 / 5592]: loss: 1.3573\n","Epoch 12 [3000 / 5592]: loss: 1.4428\n","Epoch 12 [4000 / 5592]: loss: 1.0751\n","Epoch 12 [5000 / 5592]: loss: 1.2389\n","Test accuracy: 51 %\n","Epoch 13 [0 / 5592]: loss: 2.1798\n","Epoch 13 [1000 / 5592]: loss: 0.9333\n","Epoch 13 [2000 / 5592]: loss: 1.4344\n","Epoch 13 [3000 / 5592]: loss: 1.9194\n","Epoch 13 [4000 / 5592]: loss: 1.5993\n","Epoch 13 [5000 / 5592]: loss: 0.8951\n","Test accuracy: 49 %\n","Epoch 14 [0 / 5592]: loss: 1.1782\n","Epoch 14 [1000 / 5592]: loss: 0.7669\n","Epoch 14 [2000 / 5592]: loss: 2.0805\n","Epoch 14 [3000 / 5592]: loss: 1.0521\n","Epoch 14 [4000 / 5592]: loss: 2.0956\n","Epoch 14 [5000 / 5592]: loss: 1.9404\n","Test accuracy: 50 %\n","Epoch 15 [0 / 5592]: loss: 1.0780\n","Epoch 15 [1000 / 5592]: loss: 1.2746\n","Epoch 15 [2000 / 5592]: loss: 1.6172\n","Epoch 15 [3000 / 5592]: loss: 2.8382\n","Epoch 15 [4000 / 5592]: loss: 1.0202\n","Epoch 15 [5000 / 5592]: loss: 1.1293\n","Test accuracy: 52 %\n","Epoch 16 [0 / 5592]: loss: 0.6926\n","Epoch 16 [1000 / 5592]: loss: 0.9533\n","Epoch 16 [2000 / 5592]: loss: 1.2505\n","Epoch 16 [3000 / 5592]: loss: 1.2134\n","Epoch 16 [4000 / 5592]: loss: 0.7137\n","Epoch 16 [5000 / 5592]: loss: 1.3274\n","Test accuracy: 56 %\n","Epoch 17 [0 / 5592]: loss: 0.5772\n","Epoch 17 [1000 / 5592]: loss: 1.3970\n","Epoch 17 [2000 / 5592]: loss: 0.9400\n","Epoch 17 [3000 / 5592]: loss: 0.6964\n","Epoch 17 [4000 / 5592]: loss: 1.2196\n","Epoch 17 [5000 / 5592]: loss: 0.9593\n","Test accuracy: 57 %\n","Epoch 18 [0 / 5592]: loss: 0.4146\n","Epoch 18 [1000 / 5592]: loss: 1.4195\n","Epoch 18 [2000 / 5592]: loss: 1.4086\n","Epoch 18 [3000 / 5592]: loss: 0.4763\n","Epoch 18 [4000 / 5592]: loss: 1.1976\n","Epoch 18 [5000 / 5592]: loss: 0.6350\n","Test accuracy: 57 %\n","Epoch 19 [0 / 5592]: loss: 1.4520\n","Epoch 19 [1000 / 5592]: loss: 2.0696\n","Epoch 19 [2000 / 5592]: loss: 1.1155\n","Epoch 19 [3000 / 5592]: loss: 0.8486\n","Epoch 19 [4000 / 5592]: loss: 1.3807\n","Epoch 19 [5000 / 5592]: loss: 0.9927\n","Test accuracy: 56 %\n","Epoch 20 [0 / 5592]: loss: 0.4982\n","Epoch 20 [1000 / 5592]: loss: 1.2766\n","Epoch 20 [2000 / 5592]: loss: 1.1585\n","Epoch 20 [3000 / 5592]: loss: 1.3661\n","Epoch 20 [4000 / 5592]: loss: 1.2579\n","Epoch 20 [5000 / 5592]: loss: 0.5192\n","Test accuracy: 57 %\n","Epoch 21 [0 / 5592]: loss: 1.0392\n","Epoch 21 [1000 / 5592]: loss: 1.2574\n","Epoch 21 [2000 / 5592]: loss: 0.9333\n","Epoch 21 [3000 / 5592]: loss: 1.3643\n","Epoch 21 [4000 / 5592]: loss: 1.6508\n","Epoch 21 [5000 / 5592]: loss: 0.9829\n","Test accuracy: 57 %\n","Epoch 22 [0 / 5592]: loss: 0.8889\n","Epoch 22 [1000 / 5592]: loss: 0.9578\n","Epoch 22 [2000 / 5592]: loss: 1.4735\n","Epoch 22 [3000 / 5592]: loss: 0.7767\n","Epoch 22 [4000 / 5592]: loss: 1.4241\n","Epoch 22 [5000 / 5592]: loss: 0.6361\n","Test accuracy: 58 %\n","Epoch 23 [0 / 5592]: loss: 0.4839\n","Epoch 23 [1000 / 5592]: loss: 0.9415\n","Epoch 23 [2000 / 5592]: loss: 0.5358\n","Epoch 23 [3000 / 5592]: loss: 0.9392\n","Epoch 23 [4000 / 5592]: loss: 0.5538\n","Epoch 23 [5000 / 5592]: loss: 1.2078\n","Test accuracy: 58 %\n","Epoch 24 [0 / 5592]: loss: 1.3182\n","Epoch 24 [1000 / 5592]: loss: 1.2054\n","Epoch 24 [2000 / 5592]: loss: 0.9696\n","Epoch 24 [3000 / 5592]: loss: 1.6314\n","Epoch 24 [4000 / 5592]: loss: 0.7537\n","Epoch 24 [5000 / 5592]: loss: 1.6401\n","Test accuracy: 57 %\n","Epoch 25 [0 / 5592]: loss: 0.6073\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IqymTUu0gXjv","executionInfo":{"status":"ok","timestamp":1632539616810,"user_tz":300,"elapsed":979451,"user":{"displayName":"Ting-Yu Dai","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17719512292427902131"}},"outputId":"be90375d-71f9-4b40-bed1-04bf318483df"},"source":[""],"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 21 [0 / 5592]: loss: 1.3522\n","Epoch 21 [1000 / 5592]: loss: 1.7280\n","Epoch 21 [2000 / 5592]: loss: 1.7000\n","Epoch 21 [3000 / 5592]: loss: 1.4558\n","Epoch 21 [4000 / 5592]: loss: 1.0594\n","Epoch 21 [5000 / 5592]: loss: 0.8152\n","Test accuracy: 56 %\n","Epoch 22 [0 / 5592]: loss: 2.6837\n","Epoch 22 [1000 / 5592]: loss: 1.6620\n","Epoch 22 [2000 / 5592]: loss: 1.2738\n","Epoch 22 [3000 / 5592]: loss: 0.8235\n","Epoch 22 [4000 / 5592]: loss: 0.8223\n","Epoch 22 [5000 / 5592]: loss: 0.4243\n","Test accuracy: 56 %\n","Epoch 23 [0 / 5592]: loss: 1.5869\n","Epoch 23 [1000 / 5592]: loss: 1.3389\n","Epoch 23 [2000 / 5592]: loss: 1.3605\n","Epoch 23 [3000 / 5592]: loss: 0.6173\n","Epoch 23 [4000 / 5592]: loss: 1.2221\n","Epoch 23 [5000 / 5592]: loss: 0.9207\n","Test accuracy: 56 %\n","Epoch 24 [0 / 5592]: loss: 0.6542\n","Epoch 24 [1000 / 5592]: loss: 1.3720\n","Epoch 24 [2000 / 5592]: loss: 1.2660\n","Epoch 24 [3000 / 5592]: loss: 0.7208\n","Epoch 24 [4000 / 5592]: loss: 1.6496\n","Epoch 24 [5000 / 5592]: loss: 1.4874\n","Test accuracy: 56 %\n","Epoch 25 [0 / 5592]: loss: 0.3880\n","Epoch 25 [1000 / 5592]: loss: 1.0228\n","Epoch 25 [2000 / 5592]: loss: 0.9941\n","Epoch 25 [3000 / 5592]: loss: 0.9283\n","Epoch 25 [4000 / 5592]: loss: 1.7344\n","Epoch 25 [5000 / 5592]: loss: 1.0423\n","Test accuracy: 57 %\n","Epoch 26 [0 / 5592]: loss: 0.8781\n","Epoch 26 [1000 / 5592]: loss: 1.4770\n","Epoch 26 [2000 / 5592]: loss: 1.6368\n","Epoch 26 [3000 / 5592]: loss: 1.4662\n","Epoch 26 [4000 / 5592]: loss: 1.4723\n","Epoch 26 [5000 / 5592]: loss: 0.9410\n","Test accuracy: 58 %\n","Epoch 27 [0 / 5592]: loss: 2.0430\n","Epoch 27 [1000 / 5592]: loss: 0.6457\n","Epoch 27 [2000 / 5592]: loss: 2.3165\n","Epoch 27 [3000 / 5592]: loss: 0.3884\n","Epoch 27 [4000 / 5592]: loss: 0.7466\n","Epoch 27 [5000 / 5592]: loss: 1.6388\n","Test accuracy: 57 %\n","Epoch 28 [0 / 5592]: loss: 1.2246\n","Epoch 28 [1000 / 5592]: loss: 0.7646\n","Epoch 28 [2000 / 5592]: loss: 1.3743\n","Epoch 28 [3000 / 5592]: loss: 0.9470\n","Epoch 28 [4000 / 5592]: loss: 1.1836\n","Epoch 28 [5000 / 5592]: loss: 0.8454\n","Test accuracy: 57 %\n","Epoch 29 [0 / 5592]: loss: 0.5211\n","Epoch 29 [1000 / 5592]: loss: 1.4509\n","Epoch 29 [2000 / 5592]: loss: 0.6902\n","Epoch 29 [3000 / 5592]: loss: 0.5464\n","Epoch 29 [4000 / 5592]: loss: 1.4813\n","Epoch 29 [5000 / 5592]: loss: 0.8074\n","Test accuracy: 56 %\n","Epoch 30 [0 / 5592]: loss: 0.9149\n","Epoch 30 [1000 / 5592]: loss: 0.6514\n","Epoch 30 [2000 / 5592]: loss: 1.5453\n","Epoch 30 [3000 / 5592]: loss: 0.6704\n","Epoch 30 [4000 / 5592]: loss: 2.0178\n","Epoch 30 [5000 / 5592]: loss: 1.3594\n","Test accuracy: 57 %\n","Epoch 31 [0 / 5592]: loss: 0.9826\n","Epoch 31 [1000 / 5592]: loss: 0.6916\n","Epoch 31 [2000 / 5592]: loss: 1.0946\n","Epoch 31 [3000 / 5592]: loss: 1.2180\n","Epoch 31 [4000 / 5592]: loss: 0.8986\n","Epoch 31 [5000 / 5592]: loss: 0.9386\n","Test accuracy: 56 %\n","Epoch 32 [0 / 5592]: loss: 1.6264\n","Epoch 32 [1000 / 5592]: loss: 0.5562\n","Epoch 32 [2000 / 5592]: loss: 0.4509\n","Epoch 32 [3000 / 5592]: loss: 0.6677\n","Epoch 32 [4000 / 5592]: loss: 2.2588\n","Epoch 32 [5000 / 5592]: loss: 0.7602\n","Test accuracy: 59 %\n","Epoch 33 [0 / 5592]: loss: 1.1016\n","Epoch 33 [1000 / 5592]: loss: 0.1212\n","Epoch 33 [2000 / 5592]: loss: 1.3724\n","Epoch 33 [3000 / 5592]: loss: 0.4839\n","Epoch 33 [4000 / 5592]: loss: 1.6126\n","Epoch 33 [5000 / 5592]: loss: 1.1614\n","Test accuracy: 57 %\n","Epoch 34 [0 / 5592]: loss: 0.5204\n","Epoch 34 [1000 / 5592]: loss: 2.2343\n","Epoch 34 [2000 / 5592]: loss: 1.0297\n","Epoch 34 [3000 / 5592]: loss: 0.5870\n","Epoch 34 [4000 / 5592]: loss: 1.4312\n","Epoch 34 [5000 / 5592]: loss: 0.6700\n","Test accuracy: 57 %\n","Epoch 35 [0 / 5592]: loss: 1.3121\n","Epoch 35 [1000 / 5592]: loss: 0.9202\n","Epoch 35 [2000 / 5592]: loss: 0.4962\n","Epoch 35 [3000 / 5592]: loss: 0.8591\n","Epoch 35 [4000 / 5592]: loss: 0.6730\n","Epoch 35 [5000 / 5592]: loss: 0.5066\n","Test accuracy: 59 %\n","Epoch 36 [0 / 5592]: loss: 0.8676\n","Epoch 36 [1000 / 5592]: loss: 1.3679\n","Epoch 36 [2000 / 5592]: loss: 1.6774\n","Epoch 36 [3000 / 5592]: loss: 1.8091\n","Epoch 36 [4000 / 5592]: loss: 0.5141\n","Epoch 36 [5000 / 5592]: loss: 2.2598\n","Test accuracy: 58 %\n","Epoch 37 [0 / 5592]: loss: 0.8567\n","Epoch 37 [1000 / 5592]: loss: 0.6449\n","Epoch 37 [2000 / 5592]: loss: 1.0048\n","Epoch 37 [3000 / 5592]: loss: 0.5840\n","Epoch 37 [4000 / 5592]: loss: 1.0526\n","Epoch 37 [5000 / 5592]: loss: 0.5459\n","Test accuracy: 57 %\n","Epoch 38 [0 / 5592]: loss: 1.8112\n","Epoch 38 [1000 / 5592]: loss: 1.4927\n","Epoch 38 [2000 / 5592]: loss: 1.4280\n","Epoch 38 [3000 / 5592]: loss: 1.2418\n","Epoch 38 [4000 / 5592]: loss: 2.0131\n","Epoch 38 [5000 / 5592]: loss: 0.5505\n","Test accuracy: 57 %\n","Epoch 39 [0 / 5592]: loss: 1.2205\n","Epoch 39 [1000 / 5592]: loss: 2.3114\n","Epoch 39 [2000 / 5592]: loss: 0.5690\n","Epoch 39 [3000 / 5592]: loss: 0.8234\n","Epoch 39 [4000 / 5592]: loss: 1.1511\n","Epoch 39 [5000 / 5592]: loss: 0.5495\n","Test accuracy: 56 %\n","Epoch 40 [0 / 5592]: loss: 0.5987\n","Epoch 40 [1000 / 5592]: loss: 0.5819\n","Epoch 40 [2000 / 5592]: loss: 0.6299\n","Epoch 40 [3000 / 5592]: loss: 0.3337\n","Epoch 40 [4000 / 5592]: loss: 0.2244\n","Epoch 40 [5000 / 5592]: loss: 1.6973\n","Test accuracy: 57 %\n"]}]},{"cell_type":"code","metadata":{"id":"CLAIFsa0gcNk"},"source":[""],"execution_count":null,"outputs":[]}]}